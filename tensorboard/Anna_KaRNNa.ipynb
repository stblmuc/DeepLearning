{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47, 40, 27,  2, 35,  8, 77, 19, 74, 44, 44, 44, 36, 27,  2,  2, 22,\n",
       "       19, 70, 27, 50, 69,  3, 69,  8, 24, 19, 27, 77,  8, 19, 27,  3,  3,\n",
       "       19, 27,  3, 69, 25,  8, 43, 19,  8, 21,  8, 77, 22, 19, 59, 61, 40,\n",
       "       27,  2,  2, 22, 19, 70, 27, 50, 69,  3, 22, 19, 69, 24, 19, 59, 61,\n",
       "       40, 27,  2,  2, 22, 19, 69, 61, 19, 69, 35, 24, 19, 12, 41, 61, 44,\n",
       "       41, 27, 22, 63, 44, 44, 73, 21,  8, 77, 22, 35, 40, 69, 61])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47, 40, 27,  2, 35,  8, 77, 19, 74, 44],\n",
       "       [26, 61, 57, 19, 40,  8, 19, 50, 12, 21],\n",
       "       [19, 16, 27, 35, 16, 40, 69, 61, 34, 19],\n",
       "       [12, 35, 40,  8, 77, 19, 41, 12, 59,  3],\n",
       "       [19, 35, 40,  8, 19,  3, 27, 61, 57, 55],\n",
       "       [19, 42, 40, 77, 12, 59, 34, 40, 19,  3],\n",
       "       [35, 19, 35, 12, 44, 57, 12, 63, 44, 44],\n",
       "       [12, 19, 40,  8, 77, 24,  8,  3, 70,  0],\n",
       "       [40, 27, 35, 19, 69, 24, 19, 35, 40,  8],\n",
       "       [ 8, 77, 24,  8,  3, 70, 19, 27, 61, 57]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    \n",
    "    seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                           name='softmax_w')\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syntaxfehler.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4168 7.4450 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3721 8.0760 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.1898 8.0980 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.4843 8.0820 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.4529 8.7480 sec/batch\n",
      "Epoch 1/1  Iteration 6/178 Training loss: 4.3721 10.2182 sec/batch\n",
      "Epoch 1/1  Iteration 7/178 Training loss: 4.2959 11.4445 sec/batch\n",
      "Epoch 1/1  Iteration 8/178 Training loss: 4.2189 9.2020 sec/batch\n",
      "Epoch 1/1  Iteration 9/178 Training loss: 4.1417 9.6802 sec/batch\n",
      "Epoch 1/1  Iteration 10/178 Training loss: 4.0736 10.0270 sec/batch\n",
      "Epoch 1/1  Iteration 11/178 Training loss: 4.0120 7.8865 sec/batch\n",
      "Epoch 1/1  Iteration 12/178 Training loss: 3.9601 8.6690 sec/batch\n",
      "Epoch 1/1  Iteration 13/178 Training loss: 3.9151 8.1920 sec/batch\n",
      "Epoch 1/1  Iteration 14/178 Training loss: 3.8758 7.5290 sec/batch\n",
      "Epoch 1/1  Iteration 15/178 Training loss: 3.8406 8.7580 sec/batch\n",
      "Epoch 1/1  Iteration 16/178 Training loss: 3.8087 7.8640 sec/batch\n",
      "Epoch 1/1  Iteration 17/178 Training loss: 3.7793 7.7470 sec/batch\n",
      "Epoch 1/1  Iteration 18/178 Training loss: 3.7537 8.3880 sec/batch\n",
      "Epoch 1/1  Iteration 19/178 Training loss: 3.7302 7.6520 sec/batch\n",
      "Epoch 1/1  Iteration 20/178 Training loss: 3.7068 7.3890 sec/batch\n",
      "Epoch 1/1  Iteration 21/178 Training loss: 3.6865 8.1280 sec/batch\n",
      "Epoch 1/1  Iteration 22/178 Training loss: 3.6669 8.2660 sec/batch\n",
      "Epoch 1/1  Iteration 23/178 Training loss: 3.6488 11.2870 sec/batch\n",
      "Epoch 1/1  Iteration 24/178 Training loss: 3.6321 10.0455 sec/batch\n",
      "Epoch 1/1  Iteration 25/178 Training loss: 3.6162 10.5145 sec/batch\n",
      "Epoch 1/1  Iteration 26/178 Training loss: 3.6022 9.2510 sec/batch\n",
      "Epoch 1/1  Iteration 27/178 Training loss: 3.5894 8.2931 sec/batch\n",
      "Epoch 1/1  Iteration 28/178 Training loss: 3.5758 7.7290 sec/batch\n",
      "Epoch 1/1  Iteration 29/178 Training loss: 3.5637 8.5360 sec/batch\n",
      "Epoch 1/1  Iteration 30/178 Training loss: 3.5526 8.6230 sec/batch\n",
      "Epoch 1/1  Iteration 31/178 Training loss: 3.5426 7.9570 sec/batch\n",
      "Epoch 1/1  Iteration 32/178 Training loss: 3.5325 7.4850 sec/batch\n",
      "Epoch 1/1  Iteration 33/178 Training loss: 3.5226 8.0270 sec/batch\n",
      "Epoch 1/1  Iteration 34/178 Training loss: 3.5139 7.6810 sec/batch\n",
      "Epoch 1/1  Iteration 35/178 Training loss: 3.5049 8.3950 sec/batch\n",
      "Epoch 1/1  Iteration 36/178 Training loss: 3.4968 7.9700 sec/batch\n",
      "Epoch 1/1  Iteration 37/178 Training loss: 3.4884 7.6890 sec/batch\n",
      "Epoch 1/1  Iteration 38/178 Training loss: 3.4804 7.6940 sec/batch\n",
      "Epoch 1/1  Iteration 39/178 Training loss: 3.4728 9.1855 sec/batch\n",
      "Epoch 1/1  Iteration 40/178 Training loss: 3.4655 9.0029 sec/batch\n",
      "Epoch 1/1  Iteration 41/178 Training loss: 3.4582 9.5935 sec/batch\n",
      "Epoch 1/1  Iteration 42/178 Training loss: 3.4516 10.4560 sec/batch\n",
      "Epoch 1/1  Iteration 43/178 Training loss: 3.4450 9.1685 sec/batch\n",
      "Epoch 1/1  Iteration 44/178 Training loss: 3.4387 9.4370 sec/batch\n",
      "Epoch 1/1  Iteration 45/178 Training loss: 3.4328 9.4120 sec/batch\n",
      "Epoch 1/1  Iteration 46/178 Training loss: 3.4272 9.3915 sec/batch\n",
      "Epoch 1/1  Iteration 47/178 Training loss: 3.4219 9.0890 sec/batch\n",
      "Epoch 1/1  Iteration 48/178 Training loss: 3.4170 9.2195 sec/batch\n",
      "Epoch 1/1  Iteration 49/178 Training loss: 3.4120 9.1510 sec/batch\n",
      "Epoch 1/1  Iteration 50/178 Training loss: 3.4074 7.5480 sec/batch\n",
      "Epoch 1/1  Iteration 51/178 Training loss: 3.4028 7.8570 sec/batch\n",
      "Epoch 1/1  Iteration 52/178 Training loss: 3.3981 7.5840 sec/batch\n",
      "Epoch 1/1  Iteration 53/178 Training loss: 3.3938 7.9540 sec/batch\n",
      "Epoch 1/1  Iteration 54/178 Training loss: 3.3892 7.6640 sec/batch\n",
      "Epoch 1/1  Iteration 55/178 Training loss: 3.3851 7.9470 sec/batch\n",
      "Epoch 1/1  Iteration 56/178 Training loss: 3.3809 7.8940 sec/batch\n",
      "Epoch 1/1  Iteration 57/178 Training loss: 3.3769 7.6220 sec/batch\n",
      "Epoch 1/1  Iteration 58/178 Training loss: 3.3731 8.1480 sec/batch\n",
      "Epoch 1/1  Iteration 59/178 Training loss: 3.3691 8.2190 sec/batch\n",
      "Epoch 1/1  Iteration 60/178 Training loss: 3.3655 7.8530 sec/batch\n",
      "Epoch 1/1  Iteration 61/178 Training loss: 3.3619 8.3830 sec/batch\n",
      "Epoch 1/1  Iteration 62/178 Training loss: 3.3588 8.0750 sec/batch\n",
      "Epoch 1/1  Iteration 63/178 Training loss: 3.3559 9.1860 sec/batch\n",
      "Epoch 1/1  Iteration 64/178 Training loss: 3.3523 9.3320 sec/batch\n",
      "Epoch 1/1  Iteration 65/178 Training loss: 3.3490 8.9973 sec/batch\n",
      "Epoch 1/1  Iteration 66/178 Training loss: 3.3462 9.4955 sec/batch\n",
      "Epoch 1/1  Iteration 67/178 Training loss: 3.3433 9.3813 sec/batch\n",
      "Epoch 1/1  Iteration 68/178 Training loss: 3.3398 9.8938 sec/batch\n",
      "Epoch 1/1  Iteration 69/178 Training loss: 3.3368 9.6855 sec/batch\n",
      "Epoch 1/1  Iteration 70/178 Training loss: 3.3342 9.5071 sec/batch\n",
      "Epoch 1/1  Iteration 71/178 Training loss: 3.3314 9.5550 sec/batch\n",
      "Epoch 1/1  Iteration 72/178 Training loss: 3.3290 8.9785 sec/batch\n",
      "Epoch 1/1  Iteration 73/178 Training loss: 3.3263 8.2470 sec/batch\n",
      "Epoch 1/1  Iteration 74/178 Training loss: 3.3239 8.5770 sec/batch\n",
      "Epoch 1/1  Iteration 75/178 Training loss: 3.3215 9.0580 sec/batch\n",
      "Epoch 1/1  Iteration 76/178 Training loss: 3.3192 8.9210 sec/batch\n",
      "Epoch 1/1  Iteration 77/178 Training loss: 3.3168 8.9950 sec/batch\n",
      "Epoch 1/1  Iteration 78/178 Training loss: 3.3146 7.9590 sec/batch\n",
      "Epoch 1/1  Iteration 79/178 Training loss: 3.3123 7.6160 sec/batch\n",
      "Epoch 1/1  Iteration 80/178 Training loss: 3.3098 7.7610 sec/batch\n",
      "Epoch 1/1  Iteration 81/178 Training loss: 3.3076 8.1320 sec/batch\n",
      "Epoch 1/1  Iteration 82/178 Training loss: 3.3055 8.4734 sec/batch\n",
      "Epoch 1/1  Iteration 83/178 Training loss: 3.3035 8.2761 sec/batch\n",
      "Epoch 1/1  Iteration 84/178 Training loss: 3.3014 8.6680 sec/batch\n",
      "Epoch 1/1  Iteration 85/178 Training loss: 3.2991 7.7970 sec/batch\n",
      "Epoch 1/1  Iteration 86/178 Training loss: 3.2970 7.5590 sec/batch\n",
      "Epoch 1/1  Iteration 87/178 Training loss: 3.2948 7.4550 sec/batch\n",
      "Epoch 1/1  Iteration 88/178 Training loss: 3.2928 8.1170 sec/batch\n",
      "Epoch 1/1  Iteration 89/178 Training loss: 3.2910 8.5590 sec/batch\n",
      "Epoch 1/1  Iteration 90/178 Training loss: 3.2891 8.6910 sec/batch\n",
      "Epoch 1/1  Iteration 91/178 Training loss: 3.2872 8.1730 sec/batch\n",
      "Epoch 1/1  Iteration 92/178 Training loss: 3.2852 8.1900 sec/batch\n",
      "Epoch 1/1  Iteration 93/178 Training loss: 3.2833 8.2110 sec/batch\n",
      "Epoch 1/1  Iteration 94/178 Training loss: 3.2816 7.8730 sec/batch\n",
      "Epoch 1/1  Iteration 95/178 Training loss: 3.2796 7.9160 sec/batch\n",
      "Epoch 1/1  Iteration 96/178 Training loss: 3.2777 8.8170 sec/batch\n",
      "Epoch 1/1  Iteration 97/178 Training loss: 3.2760 8.5690 sec/batch\n",
      "Epoch 1/1  Iteration 98/178 Training loss: 3.2770 7.6360 sec/batch\n",
      "Epoch 1/1  Iteration 99/178 Training loss: 3.2882 7.4410 sec/batch\n",
      "Epoch 1/1  Iteration 100/178 Training loss: 3.2977 7.5910 sec/batch\n",
      "Epoch 1/1  Iteration 101/178 Training loss: 3.3036 7.5040 sec/batch\n",
      "Epoch 1/1  Iteration 102/178 Training loss: 3.3061 8.3570 sec/batch\n",
      "Epoch 1/1  Iteration 103/178 Training loss: 3.3050 8.0960 sec/batch\n",
      "Epoch 1/1  Iteration 104/178 Training loss: 3.3036 8.5470 sec/batch\n",
      "Epoch 1/1  Iteration 105/178 Training loss: 3.3021 8.0550 sec/batch\n",
      "Epoch 1/1  Iteration 106/178 Training loss: 3.3006 8.5200 sec/batch\n",
      "Epoch 1/1  Iteration 107/178 Training loss: 3.2989 8.5400 sec/batch\n",
      "Epoch 1/1  Iteration 108/178 Training loss: 3.2972 8.1050 sec/batch\n",
      "Epoch 1/1  Iteration 109/178 Training loss: 3.2957 8.6690 sec/batch\n",
      "Epoch 1/1  Iteration 110/178 Training loss: 3.2940 8.7310 sec/batch\n",
      "Epoch 1/1  Iteration 111/178 Training loss: 3.2924 8.3850 sec/batch\n",
      "Epoch 1/1  Iteration 112/178 Training loss: 3.2908 8.2550 sec/batch\n",
      "Epoch 1/1  Iteration 113/178 Training loss: 3.2891 8.0300 sec/batch\n",
      "Epoch 1/1  Iteration 114/178 Training loss: 3.2873 8.4457 sec/batch\n",
      "Epoch 1/1  Iteration 115/178 Training loss: 3.2855 9.7760 sec/batch\n",
      "Epoch 1/1  Iteration 116/178 Training loss: 3.2838 9.9146 sec/batch\n",
      "Epoch 1/1  Iteration 117/178 Training loss: 3.2821 9.2625 sec/batch\n",
      "Epoch 1/1  Iteration 118/178 Training loss: 3.2805 9.4260 sec/batch\n",
      "Epoch 1/1  Iteration 119/178 Training loss: 3.2790 9.4476 sec/batch\n",
      "Epoch 1/1  Iteration 120/178 Training loss: 3.2773 9.6285 sec/batch\n",
      "Epoch 1/1  Iteration 121/178 Training loss: 3.2758 9.1610 sec/batch\n",
      "Epoch 1/1  Iteration 122/178 Training loss: 3.2741 9.4130 sec/batch\n",
      "Epoch 1/1  Iteration 123/178 Training loss: 3.2724 9.3708 sec/batch\n",
      "Epoch 1/1  Iteration 124/178 Training loss: 3.2709 9.4067 sec/batch\n",
      "Epoch 1/1  Iteration 125/178 Training loss: 3.2690 8.9230 sec/batch\n",
      "Epoch 1/1  Iteration 126/178 Training loss: 3.2671 9.7613 sec/batch\n",
      "Epoch 1/1  Iteration 127/178 Training loss: 3.2653 9.2025 sec/batch\n",
      "Epoch 1/1  Iteration 128/178 Training loss: 3.2637 9.3840 sec/batch\n",
      "Epoch 1/1  Iteration 129/178 Training loss: 3.2618 9.5450 sec/batch\n",
      "Epoch 1/1  Iteration 130/178 Training loss: 3.2602 9.1450 sec/batch\n",
      "Epoch 1/1  Iteration 131/178 Training loss: 3.2585 8.3850 sec/batch\n",
      "Epoch 1/1  Iteration 132/178 Training loss: 3.2566 9.9879 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
