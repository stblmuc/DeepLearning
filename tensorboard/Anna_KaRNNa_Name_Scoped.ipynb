{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58, 62,  1, 31, 12, 52, 42, 68, 27, 82, 82, 82, 36,  1, 31, 31,  8,\n",
       "       68, 76,  1, 69,  7, 63,  7, 52, 25, 68,  1, 42, 52, 68,  1, 63, 63,\n",
       "       68,  1, 63,  7, 43, 52, 78, 68, 52, 57, 52, 42,  8, 68, 11, 60, 62,\n",
       "        1, 31, 31,  8, 68, 76,  1, 69,  7, 63,  8, 68,  7, 25, 68, 11, 60,\n",
       "       62,  1, 31, 31,  8, 68,  7, 60, 68,  7, 12, 25, 68, 14, 51, 60, 82,\n",
       "       51,  1,  8, 55, 82, 82, 53, 57, 52, 42,  8, 12, 62,  7, 60])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[58, 62,  1, 31, 12, 52, 42, 68, 27, 82],\n",
       "       [70, 60, 45, 68, 62, 52, 68, 69, 14, 57],\n",
       "       [68, 39,  1, 12, 39, 62,  7, 60, 19, 68],\n",
       "       [14, 12, 62, 52, 42, 68, 51, 14, 11, 63],\n",
       "       [68, 12, 62, 52, 68, 63,  1, 60, 45, 64],\n",
       "       [68, 40, 62, 42, 14, 11, 19, 62, 68, 63],\n",
       "       [12, 68, 12, 14, 82, 45, 14, 55, 82, 82],\n",
       "       [14, 68, 62, 52, 42, 25, 52, 63, 76, 29],\n",
       "       [62,  1, 12, 68,  7, 25, 68, 12, 62, 52],\n",
       "       [52, 42, 25, 52, 63, 76, 68,  1, 60, 45]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/3', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syntaxfehler.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4196 9.1916 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3819 9.0185 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.2498 9.0769 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.5555 9.0780 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.4995 7.8506 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.3735 7.8660 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 4.2705 8.6890 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 4.1845 7.9370 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 4.1073 8.8060 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 4.0417 9.3690 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.9847 8.2960 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.9326 8.4860 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.8858 7.9870 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.8459 8.0850 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.8102 8.3530 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.7781 8.5740 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.7482 8.0830 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.7234 7.6140 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.6999 8.1970 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.6762 8.2770 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.6560 8.3610 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.6368 8.5230 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.6190 8.6920 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.6025 8.9686 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.5870 9.2526 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.5731 9.4339 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.5604 8.9945 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.5472 8.9725 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.5354 9.0065 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.5244 8.7880 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.5147 9.3665 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.5046 9.5166 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.4948 9.5602 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.4862 9.0685 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.4774 8.7120 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.4698 8.9758 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.4615 8.8385 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.4537 8.7990 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.4461 8.8787 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.4391 8.9640 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.4322 9.8348 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.4257 7.6655 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.4192 7.4990 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.4132 7.6800 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.4072 7.6760 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.4019 7.6770 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.3968 7.6710 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3919 7.6750 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3873 7.9280 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3827 7.8580 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3782 7.9460 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3737 8.0960 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3694 7.8430 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3650 7.5750 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.3612 7.6490 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.3570 7.5620 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.3532 7.6020 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.3496 7.6860 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.3459 7.6990 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.3425 7.6670 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.3392 7.6460 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.3363 7.6960 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.3335 7.7670 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.3301 7.7230 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.3270 7.7460 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.3242 7.7520 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.3214 7.6000 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.3181 8.1090 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.3152 8.1140 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.3126 7.6490 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.3099 7.7040 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.3075 7.5630 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.3049 7.9980 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.3025 7.5970 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.3002 7.5410 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.2979 7.6950 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.2957 7.5700 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.2934 7.6030 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.2911 7.5710 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2886 7.6060 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2862 7.7760 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2841 7.6530 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2821 7.6840 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2799 7.6020 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2776 7.5960 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2755 8.0600 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2734 7.7250 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2712 7.5790 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2693 7.6890 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2673 7.6050 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2654 7.6470 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2633 7.5720 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2614 7.5920 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2595 7.5350 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2574 7.5520 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2554 7.5690 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2535 7.4740 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2515 7.5290 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2497 7.7290 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2479 7.5570 sec/batch\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2462 7.5610 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2444 7.7550 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2427 7.8060 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2408 7.6570 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2390 7.6420 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2372 7.7210 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.2352 7.9800 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.2332 8.1440 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.2314 7.8390 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.2291 7.6740 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.2272 7.7470 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.2252 7.6790 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.2230 7.7620 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.2230 7.6440 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.2226 7.5710 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.2207 7.7220 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.2188 7.7580 sec/batch\n",
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.2170 7.7360 sec/batch\n",
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.2154 7.7180 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.2135 7.5990 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.2118 7.6740 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.2100 7.7010 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.2081 7.7070 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.2074 7.6060 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.2071 7.6720 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.2064 7.6760 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.2055 7.5930 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.2046 7.6330 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.2031 8.2440 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.2012 7.5760 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.1993 7.6600 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.1971 7.7320 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.1950 7.9230 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.1928 7.6110 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.1904 7.7150 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.1881 7.6380 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.1859 7.6730 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.1837 7.6530 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.1816 7.7570 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.1794 7.6070 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.1772 7.6460 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.1749 7.6940 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.1726 7.7480 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.1703 7.6590 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.1680 7.7220 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1658 8.1830 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1635 7.8990 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1613 7.7430 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.1588 7.9620 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.1564 7.7690 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.1542 7.5610 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.1520 7.7520 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.1496 7.6830 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.1472 7.7250 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.1447 7.7040 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.1421 7.5400 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.1395 7.6450 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.1369 7.7460 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.1342 7.6190 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.1316 7.6390 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.1290 7.7490 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.1261 7.7190 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.1233 7.9640 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.1206 7.6840 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.1180 7.7680 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.1153 7.6620 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.1126 7.6040 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.1099 7.6560 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.1072 7.7700 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.1044 7.7030 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.1016 7.6270 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.0990 7.6840 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.0965 7.8710 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.0940 7.6840 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.0913 7.5830 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.0886 7.8420 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.0857 7.6710 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.0828 7.6900 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.6282 7.7560 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.5905 7.9610 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.5796 7.9050 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.5747 8.0030 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.5698 7.8700 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.5654 8.2460 sec/batch\n",
      "Epoch 2/10  Iteration 185/1780 Training loss: 2.5623 8.0080 sec/batch\n",
      "Epoch 2/10  Iteration 186/1780 Training loss: 2.5606 8.2680 sec/batch\n",
      "Epoch 2/10  Iteration 187/1780 Training loss: 2.5603 7.5890 sec/batch\n",
      "Epoch 2/10  Iteration 188/1780 Training loss: 2.5567 7.8610 sec/batch\n",
      "Epoch 2/10  Iteration 189/1780 Training loss: 2.5537 7.6760 sec/batch\n",
      "Epoch 2/10  Iteration 190/1780 Training loss: 2.5524 7.5360 sec/batch\n",
      "Epoch 2/10  Iteration 191/1780 Training loss: 2.5500 7.7290 sec/batch\n",
      "Epoch 2/10  Iteration 192/1780 Training loss: 2.5505 7.6680 sec/batch\n",
      "Epoch 2/10  Iteration 193/1780 Training loss: 2.5482 7.5660 sec/batch\n",
      "Epoch 2/10  Iteration 194/1780 Training loss: 2.5464 7.7060 sec/batch\n",
      "Epoch 2/10  Iteration 195/1780 Training loss: 2.5445 7.8500 sec/batch\n",
      "Epoch 2/10  Iteration 196/1780 Training loss: 2.5449 7.8150 sec/batch\n",
      "Epoch 2/10  Iteration 197/1780 Training loss: 2.5430 7.5970 sec/batch\n",
      "Epoch 2/10  Iteration 198/1780 Training loss: 2.5397 7.5610 sec/batch\n",
      "Epoch 2/10  Iteration 199/1780 Training loss: 2.5372 7.7590 sec/batch\n",
      "Epoch 2/10  Iteration 200/1780 Training loss: 2.5366 7.6870 sec/batch\n",
      "Validation loss: 2.41144 Saving checkpoint!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of checkpoints/anna/i200_l512_2.411.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-09fa3beeed23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 print('Validation loss:', np.mean(val_loss),\n\u001b[1;32m     57\u001b[0m                       'Saving checkpoint!')\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\sbl\\AppData\\Local\\Continuum\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m       raise ValueError(\n\u001b[0;32m-> 1365\u001b[0;31m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of checkpoints/anna/i200_l512_2.411.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
