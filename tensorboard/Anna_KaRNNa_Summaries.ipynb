{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 29,  5, 10, 80, 25, 14, 71, 48, 33, 33, 33,  0,  5, 10, 10, 60,\n",
       "       71, 17,  5, 78,  8, 24,  8, 25, 26, 71,  5, 14, 25, 71,  5, 24, 24,\n",
       "       71,  5, 24,  8, 55, 25, 70, 71, 25, 82, 25, 14, 60, 71, 19, 61, 29,\n",
       "        5, 10, 10, 60, 71, 17,  5, 78,  8, 24, 60, 71,  8, 26, 71, 19, 61,\n",
       "       29,  5, 10, 10, 60, 71,  8, 61, 71,  8, 80, 26, 71, 65, 77, 61, 33,\n",
       "       77,  5, 60,  7, 33, 33, 74, 82, 25, 14, 60, 80, 29,  8, 61])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 29,  5, 10, 80, 25, 14, 71, 48, 33],\n",
       "       [ 2, 61,  9, 71, 29, 25, 71, 78, 65, 82],\n",
       "       [71, 32,  5, 80, 32, 29,  8, 61, 52, 71],\n",
       "       [65, 80, 29, 25, 14, 71, 77, 65, 19, 24],\n",
       "       [71, 80, 29, 25, 71, 24,  5, 61,  9, 46],\n",
       "       [71, 43, 29, 14, 65, 19, 52, 29, 71, 24],\n",
       "       [80, 71, 80, 65, 33,  9, 65,  7, 33, 33],\n",
       "       [65, 71, 29, 25, 14, 26, 25, 24, 17, 30],\n",
       "       [29,  5, 80, 71,  8, 26, 71, 80, 29, 25],\n",
       "       [25, 14, 26, 25, 24, 17, 71,  5, 61,  9]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "    \n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    \n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output\n",
    "    with tf.name_scope('sequence_reshape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syntaxfehler.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4183 7.6810 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3798 7.7650 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.2273 7.9570 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.5075 8.5190 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.4054 8.7870 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.3222 7.7240 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 4.2438 7.5280 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 4.1667 8.1330 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 4.0903 7.5580 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 4.0211 8.0310 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.9618 9.8522 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.9120 11.2452 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.8687 12.1389 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.8322 9.7015 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.7994 9.5808 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.7693 9.7694 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.7419 10.1845 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.7178 8.7278 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.6951 7.4610 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.6727 7.5440 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.6529 7.4870 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.6343 7.5070 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.6174 7.6120 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.6015 8.4290 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.5864 8.0680 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.5731 7.5770 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.5607 7.5200 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.5480 7.4770 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.5366 7.5880 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.5258 7.5410 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.5163 7.4470 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.5066 7.4370 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.4971 7.6110 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.4887 7.5140 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.4800 7.3520 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.4721 7.5050 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.4638 7.5750 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.4562 7.4880 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.4489 7.4970 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.4419 7.5100 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.4350 7.5970 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.4287 7.4070 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.4225 7.5530 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.4164 7.5720 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.4105 7.5300 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.4052 7.4300 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.4002 7.5940 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3954 7.4860 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3907 7.8070 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3862 8.4880 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3818 7.9880 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3774 9.6230 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3733 7.7900 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3690 9.0310 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.3651 8.1240 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.3609 8.3940 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.3571 8.4880 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.3534 8.2924 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.3497 8.3216 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.3463 8.2054 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.3430 8.2256 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.3401 8.4030 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.3373 7.5320 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.3340 7.5310 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.3308 7.4950 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.3281 7.5940 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.3254 7.4870 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.3221 7.4520 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.3191 7.5390 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.3165 7.4760 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.3139 7.4320 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.3116 7.5550 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.3091 7.5720 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.3067 7.4220 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.3045 7.4490 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.3024 7.6940 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.3002 7.5070 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.2980 8.3050 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.2958 8.4810 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2935 8.5300 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2912 7.8720 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2892 8.9780 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2872 7.9670 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2852 7.6300 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2831 7.8540 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2810 7.6950 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2789 8.8180 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2769 7.7140 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2752 7.5720 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2735 7.7839 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2718 7.6880 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2699 7.5610 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2682 7.7210 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2665 7.5650 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2647 8.4810 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2629 7.7690 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2613 7.4560 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2596 7.4540 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2579 7.5430 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2563 7.5310 sec/batch\n",
      "Validation loss: 3.05691 Saving checkpoint!\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2546 7.6280 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2530 7.5630 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2513 7.5590 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2497 7.4730 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2479 7.4390 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2463 7.5040 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.2445 7.6020 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.2427 7.5300 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.2410 7.4550 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.2390 7.5390 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.2382 7.6410 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.2375 7.5260 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.2364 7.5730 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.2350 7.4520 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.2334 7.5690 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.2319 7.4610 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.2305 7.6940 sec/batch\n",
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.2292 7.6290 sec/batch\n",
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.2280 8.0090 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.2265 7.8270 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.2253 7.6340 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.2239 7.6040 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.2224 7.6160 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.2211 8.3760 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.2196 7.6390 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.2180 8.2200 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.2166 8.8517 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.2152 11.3285 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.2137 9.3681 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.2122 9.0318 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.2108 9.3570 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.2092 10.1593 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.2077 9.2120 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.2061 8.9413 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.2043 9.4830 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.2025 9.0215 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.2008 9.0042 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.1990 8.8085 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.1973 9.2242 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.1956 8.9178 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.1938 8.9695 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.1919 8.8405 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.1900 9.0760 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.1881 9.0755 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.1864 10.3148 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1850 9.5584 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1836 9.7753 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1822 9.6750 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.1804 9.5383 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.1787 8.7588 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.1772 8.0380 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.1757 8.4770 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.1740 9.5530 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.1724 8.6760 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.1705 9.4550 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.1687 8.4840 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.1667 8.5200 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.1648 8.4580 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.1627 7.7120 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.1607 12.7577 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.1587 9.5335 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.1565 12.4232 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.1542 10.6850 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.1520 9.7776 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.1498 10.2964 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.1476 10.1667 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.1454 9.6935 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.1432 9.5890 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.1410 9.4948 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.1386 9.4725 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.1364 10.0356 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.1342 10.4331 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.1321 9.5320 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.1299 9.9997 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.1278 10.5557 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.1263 10.2580 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.1239 10.4113 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.1214 10.5845 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.7412 10.3336 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.6952 9.0130 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.6786 10.3926 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.6727 10.8756 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.6658 10.7931 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.6612 10.3745 sec/batch\n",
      "Epoch 2/10  Iteration 185/1780 Training loss: 2.6571 8.8517 sec/batch\n",
      "Epoch 2/10  Iteration 186/1780 Training loss: 2.6544 7.5075 sec/batch\n",
      "Epoch 2/10  Iteration 187/1780 Training loss: 2.6516 7.6360 sec/batch\n",
      "Epoch 2/10  Iteration 188/1780 Training loss: 2.6468 7.5370 sec/batch\n",
      "Epoch 2/10  Iteration 189/1780 Training loss: 2.6422 7.6810 sec/batch\n",
      "Epoch 2/10  Iteration 190/1780 Training loss: 2.6397 7.7180 sec/batch\n",
      "Epoch 2/10  Iteration 191/1780 Training loss: 2.6362 7.5470 sec/batch\n",
      "Epoch 2/10  Iteration 192/1780 Training loss: 2.6354 7.6690 sec/batch\n",
      "Epoch 2/10  Iteration 193/1780 Training loss: 2.6321 7.4690 sec/batch\n",
      "Epoch 2/10  Iteration 194/1780 Training loss: 2.6288 7.5280 sec/batch\n",
      "Epoch 2/10  Iteration 195/1780 Training loss: 2.6256 7.5660 sec/batch\n",
      "Epoch 2/10  Iteration 196/1780 Training loss: 2.6244 7.6320 sec/batch\n",
      "Epoch 2/10  Iteration 197/1780 Training loss: 2.6212 7.5080 sec/batch\n",
      "Epoch 2/10  Iteration 198/1780 Training loss: 2.6171 7.5460 sec/batch\n",
      "Epoch 2/10  Iteration 199/1780 Training loss: 2.6135 7.5950 sec/batch\n",
      "Epoch 2/10  Iteration 200/1780 Training loss: 2.6116 7.6880 sec/batch\n",
      "Validation loss: 2.46213 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 201/1780 Training loss: 2.6093 7.6900 sec/batch\n",
      "Epoch 2/10  Iteration 202/1780 Training loss: 2.6060 7.6820 sec/batch\n",
      "Epoch 2/10  Iteration 203/1780 Training loss: 2.6023 7.6450 sec/batch\n",
      "Epoch 2/10  Iteration 204/1780 Training loss: 2.5996 7.4870 sec/batch\n",
      "Epoch 2/10  Iteration 205/1780 Training loss: 2.5964 7.5610 sec/batch\n",
      "Epoch 2/10  Iteration 206/1780 Training loss: 2.5934 8.1490 sec/batch\n",
      "Epoch 2/10  Iteration 207/1780 Training loss: 2.5908 8.4800 sec/batch\n",
      "Epoch 2/10  Iteration 208/1780 Training loss: 2.5883 11.5367 sec/batch\n",
      "Epoch 2/10  Iteration 209/1780 Training loss: 2.5863 11.5416 sec/batch\n",
      "Epoch 2/10  Iteration 210/1780 Training loss: 2.5833 11.2817 sec/batch\n",
      "Epoch 2/10  Iteration 211/1780 Training loss: 2.5800 10.6628 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged, model.cost, \n",
    "                                                          model.final_state, model.optimizer], \n",
    "                                                          feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                    \n",
    "                test_writer.add_summary(summary, iteration)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                #saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
